---
title: "Twitter Analysis"
author: "David Rodriguez"
date: "February 24, 2016"
output: html_document
---

## Introduction

Twitter is a powerful tool that enables users to communicate with others and also empowers data scientists with large quantities of data they can use. Communications on twitter are live and dynamic, changing every second. They are also short, limited to 140 characters. The analysis of such short, fast-moving data can reveal how people communicate on a particular subject.

For this analysis, I have chosen to gather English-language tweets containing the word 'microsoft'. This allows me to explore how people regard the microsoft brand, as well as how they treat any news articles referring to microsoft. In practice, this methodology can be employed to any other broadly tweeted topic, including politics (eg, presidential candidates) and popular culture (eg, the Academy Awards).

The ultimate goal of any analysis is to be able to predict some quantity of interest. For my purposes, I wished to examine if I could predict how users would tweet about the specified subject based on information publicly available from Twitter.

## Experiment Setup

Load up required packages:
```{r message=F, warning=F}
library(twitteR)
library(tm)
library(rjson)
library(wordcloud)
library(dplyr)
library(caret)
library(knitr)
library(RColorBrewer)
library(stringr)
library(syuzhet) # for sentiment analysis
library(rattle)
library(lubridate)
library(rpart)
```

To access twitter, I need to provide authorization credentials for my Twitter application:
```{r eval=T}
secrets <- fromJSON(file='twitter_secrets.json.nogit')

setup_twitter_oauth(secrets$api_key,
                    secrets$api_secret,
                    secrets$access_token,
                    secrets$access_token_secret)
```

Perform a twitter search and extract the information I want:
```{r eval=F}
searchstring <- 'microsoft'
numtweets <- 10000
st <- searchTwitter(searchstring, n=numtweets, resultType = 'recent', lang = 'en')

statuses <- data.frame(text=sapply(st, function(x) x$getText()),
                       user=sapply(st, function(x) x$getScreenName()),
                       RT=sapply(st, function(x) x$isRetweet),
                       latitude=sapply(st, function(x) as.numeric(x$latitude[1])),
                       longitude=sapply(st, function(x) as.numeric(x$longitude[1])),
                       time=sapply(st, function(x) format(x$created, format='%F %T'))
                       )
```

Remove retweets for clarity:
```{r eval=F}
statuses <-
    statuses %>%
    filter(!RT)
```

Save tweets for future use:
```{r savetweets, eval=F}
today <- format(Sys.time(), '%Y-%m-%d')
savename <- paste0('data/tweets_',searchstring,'_',
                   nrow(statuses),'_',today,'.Rda')
saveRDS(statuses, file=savename)
```

Alternatively, I load up prior searches to avoid re-running:
```{r loadtweets, eval=T}
files <- list.files('data','tweets_')
searchstring <- 'microsoft'
rm(statuses)
for(i in 1:length(files)) {
    selectedfile <- paste0('data/',files[i])
    print(selectedfile)
    if(!exists('statuses')){
        statuses <- readRDS(file=selectedfile)
    }else{
        statuses <- rbind(statuses, readRDS(file=selectedfile))
    }
}
```

Total number of tweets to process is `r nrow(statuses)`

## Text Analysis

Gather the tweets:
```{r gathercorpus, cache=T}
textdata <- Corpus(VectorSource(statuses$text))

textdata <- 
    textdata %>%
    tm_map(removeWords, stopwords("english"), mc.cores=1) %>%
    tm_map(removePunctuation, mc.cores=1) %>%
    tm_map(content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
           mc.cores=1) %>%
    tm_map(content_transformer(tolower), mc.cores=1) %>%
    tm_map(content_transformer(function(x) str_replace_all(x, "@\\w+", "")), 
           mc.cores=1) %>% # remove twitter handles
    tm_map(removeNumbers, mc.cores=1) %>%
    tm_map(removeWords, c('trump','realdonaldtrump'), mc.cores=1) %>%
    tm_map(stemDocument) %>%
    tm_map(stripWhitespace, mc.cores=1)

save(textdata, file = 'data/testdata_corpus.RData') 
rm(textdata)
```

```{r loadcorpus, cache=F}
load('data/testdata_corpus.RData') 
```


I also perform a sentiment analysis on the text data by comparing the words with those from the [NRC Word-Emotion Association Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), which assigns them two 8 emotions (eg, anger, joy, etc) and 2 sentiments (postive and negative). I create a new variable, positivity, which is the difference between the positive and negative sentiments.
```{r sentiment, eval=T, cache=T}
sentiments <- sapply(textdata, function(x) get_nrc_sentiment(as.character(x)))

sentiments <- as.data.frame(aperm(sentiments)) # transpose and save as dataframe
sentiments <- as.data.frame(lapply(sentiments, as.numeric)) # a bit more to organize
sentiments <-
    sentiments %>%
    mutate(positivity = positive - negative)
```

A quick wordcloud of the tweets reveals the 100 key words used:
```{r wordcloud}
pal2 <- brewer.pal(8,"Dark2")
wordcloud(textdata, max.words = 100, colors= pal2, random.order=F, 
          rot.per=0.1, use.r.layout=F)
```

Further processing to get word counts:
```{r termmatrix, message=F, results='hide', cache=T}
dtm <- DocumentTermMatrix(textdata)
dtm <- inspect(dtm)

save(dtm, file = 'data/DocumentTermMatrix.RData') 
rm(dtm)
```

```{r loadmatrix, cache=F}
load('data/DocumentTermMatrix.RData') 
```

Sort in descending order to find the most common terms:
```{r cache=F}
words <- data.frame(term = colnames(dtm))
words$count <- colSums(dtm)

words <-
    words %>%
    arrange(desc(count))
head(words)
```

Convert tweets to data frame and select only the top 100 words to process:
```{r tweets_frame, cache=T}
tweets <- as.data.frame(dtm)
ind <- data.frame('id'=seq.int(nrow(tweets)))
tweets <- cbind(ind, tweets)

words_100 <- as.character(words[2:101,'term'])
tweets <- tweets[,c('id',words_100)]

save(tweets, file = 'data/tweets.RData') 
rm(tweets, dtm)
```

```{r tweets_load, cache=F}
rm(dtm, textdata)
load('data/tweets.RData') 
```

## Principal Component Analysis

Perform a principal component analysis on the tweet data set and join the information (first 5 components) to the original status array.
```{r runpca, cache=T}
trans <- preProcess(tweets[,2:ncol(tweets)], method=c("pca"), thresh = 0.95)
pca <- predict(trans, tweets[,2:ncol(tweets)])
statuses <- cbind(statuses, pca[,1:5], sentiments)
```

I now examine the reprojected data:
```{r pca1-2_plot}
pal2 <- brewer.pal(10,"RdBu")
ggplot(statuses, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=positivity), size=4, alpha=0.7, pch=21, stroke=1.3) + 
    scale_fill_gradientn(colours = pal2, limits=c(-5,5)) + theme_bw()
```

Sometimes the other principal components are more illustrative:
```{r pca2-3_plot}
pal2 <- brewer.pal(10,"RdBu")
ggplot(statuses, aes(x=PC2, y=PC3)) + 
    geom_point(aes(fill=positivity), size=4, alpha=0.7, pch=21, stroke=1.3) + 
    scale_fill_gradientn(colours = pal2, limits=c(-5,5)) + theme_bw()
```

I remove outliers, which I define as the 2% highest and lowest PC values in terms of PC1 and PC2:
```{r outlier_cuts, eval=T}
cutlevel <- 2/100.
cut1 <- quantile(statuses$PC1, probs=c(cutlevel,1-cutlevel))
cut2 <- quantile(statuses$PC2, probs=c(cutlevel,1-cutlevel))

statuses <- 
    statuses %>%
    filter(PC1>cut1[1] & PC1<cut1[2]) %>%
    filter(PC2>cut2[1] & PC2<cut2[2])
```


The loading factors reveal how important each term is to the principal component axes. Here are the top few terms of the first two components:
```{r}
loadings <- trans$rotation 
load_sqr <- loadings^2

load_sqr <- data.frame(load_sqr)
temp <- data.frame('term'=rownames(load_sqr))
load_sqr <- cbind(temp, load_sqr)
load_sqr %>%
    select(term, PC1) %>%
    arrange(desc(PC1)) %>%
    head(10) %>% kable

load_sqr %>%
    select(term, PC2) %>%
    arrange(desc(PC2)) %>%
    head(10) %>% kable
```

I've created a function to sample tweets across the PC spectrum.
```{r}
set.seed(42)
tweet_check <- function(text, pc, numbreaks=5){
    cuts <- cut(pc, numbreaks)
    #cuts <- cut(pc, breaks=quantile(pc, probs=seq(0,1,1/numbreaks)))
    temp <- data.frame(text=text, pc=pc, pc_val=cuts)
    temp <- temp %>%
        group_by(pc_val) %>%
        summarise(text=iconv(sample(text,1), to='UTF-8-MAC', sub='byte')) %>%
        filter(!is.na(pc_val))
    return(temp)
}
```

Here are the results for PC1:
```{r}
tweet_check(statuses$text, statuses$PC1, 10) %>% kable(format='html')
```

And here are the results for PC2:
```{r}
set.seed(42)
tweet_check(statuses$text, statuses$PC2, 10) %>% kable(format='html')
```

Finally, let's have a look at tweets grouped by positivity score:
```{r}
tweet_check(statuses$text, statuses$positivity, 10) %>% kable(format='html')
```

## Gather User Data

Now, I gather all the user data for each particular tweet.
```{r gather_users, cache=T, eval=F}
userlist <- sapply(unique(statuses$user), as.character)
allusers <- lookupUsers(userlist)

# Gather all the user info in a data frame
userinfo <- data.frame(user=sapply(allusers, function(x) x$screenName),
                       realname=sapply(allusers, function(x) x$name),
                       numstatuses=sapply(allusers, function(x) x$statusesCount),
                       followers=sapply(allusers, function(x) x$followersCount),
                       friends=sapply(allusers, function(x) x$friendsCount),
                       favorites=sapply(allusers, function(x) x$favoritesCount),
                       account_created=sapply(allusers, function(x) format(x$created, 
                                                              format='%F %T')),
                       verified=sapply(allusers, function(x) x$verified),
                       numlists=sapply(allusers, function(x) x$listedCount)) %>%
    mutate(user=as.character(user)) %>%
    mutate(twitter_years=interval(account_created, Sys.time()) / dyears(1)) %>%
    select(-account_created)

save(userinfo, file='data/userinfo.Rdata')
#rm(userinfo)
```

```{r load_users, cache=F}
load('data/userinfo.Rdata')
```

The original tweets are grouped together by user (taking averages of the relevant quantities of interest) and then joined together to the user information data frame:
```{r jointables, cache=F}
newstatuses <-
    statuses %>%
    group_by(user) %>%
    summarize(numTopicTweets=n(),
              positivity=mean(positivity),
              PC1=mean(PC1),
              PC2=mean(PC2),
              PC3=mean(PC3),
              PC4=mean(PC4),
              PC5=mean(PC5),
              client=rownames(sort(table(client), decreasing = T))[1],
              anger=mean(anger), anticipation=mean(anticipation), 
              disgust=mean(disgust), fear=mean(fear), joy=mean(joy),
              sadness=mean(sadness), surprise=mean(surprise), trust=mean(trust)) %>% 
    mutate(user=as.character(user))

# Join the data together
alldata <- inner_join(userinfo, newstatuses, by='user')
```

## Predictive Analysis

I'll now decide on the quantity to predict and clean up the table to be able to run my models.
I'll choose to examine the 2nd principal component, but this can readily be changed to the other components or the positivity.
```{r}
possiblepreds <- c('PC1','PC2','PC3','PC4','PC5')
choice <- 'PC2'

# Remove columns
if(T){
    colremove <- possiblepreds[sapply(possiblepreds, function(x) x!=choice)]
    col_list <- colnames(alldata)
    final_cols <- col_list[sapply(col_list, function(x) !(x %in% colremove))]
    df <- alldata[final_cols]
}else{
    df <- alldata
}

df <- df %>% 
    select(-user, -realname, -client) %>%
    select(-(anger:trust)) # removing sentiment columns
```

In the event that there are parameters with little variance, I remove them:
```{r}
nzv <- nearZeroVar(df)
df_filter <- df[, -nzv]
df_filter <- na.omit(df_filter)
```

I now prepare my training and test data sets:
```{r}
set.seed(3456)
trainIndex <- sample(nrow(df_filter), nrow(df_filter)*0.8)

df_train <- df_filter[ trainIndex,]
df_test  <- df_filter[-trainIndex,]
```

First, I'll run a regression tree model:
```{r run_rpart, cache=F, message=F, results='hide'}
rtGrid <- expand.grid(cp=seq(0.005, 0.1, by = 0.005)) # grid of cp values
ctrl <- trainControl(method = "cv", number = 10, verboseIter = T)

toRun <- formula(paste0(choice,' ~ .'))
rtTune <- train(toRun, data = df_train, method = "rpart", 
                tuneGrid = rtGrid, trControl = ctrl)
```

Here are the results for my regression tree model:
```{r rt_plot}
fancyRpartPlot(rtTune$finalModel, palettes=c("Blues"), sub='')
```

The numeric values of our best fit are saved for later comparison:
```{r}
df_test_all <- df_test
df_test_all[,'id'] <- seq(nrow(df_test_all))
pr_rt <- predict(rtTune, newdata = df_test)
rmseTree <- RMSE(pr_rt, df_test[,choice])
modelSummary <- data.frame(model='Regression Tree',
                         RMSE=rmseTree)

df_test_all[,'diff_Tree'] = df_test_all[,choice] - pr_rt
```

I next run a generalized linear model:
```{r run_glm, cache=F, message=F, results='hide'}
ctrl <- trainControl(method = "cv", number = 10, verboseIter = T)
    
toRun <- formula(paste0(choice,' ~ .'))
rtTune <- train(toRun, data = df_train,   
                method = "glm", 
                trControl = ctrl)
```

Here are the best-fit model parameters:
```{r}
summary(rtTune)
```

As before, we save the results for future comparison:
```{r}
pr_rt <- predict(rtTune, newdata = df_test)
rmseGLM <- RMSE(pr_rt, df_test[,choice])
    
newRow <- data.frame(model='Generalized LM',
                     RMSE=rmseGLM)
modelSummary <- rbind(modelSummary, newRow)
    
df_test_all[,'diff_GLM'] = df_test_all[,choice] - pr_rt
```

## Model Comparison

The figure below compares the result of the various models:
```{r rmse_plot}
ggplot(data=df_test_all, aes(x=id)) +
    geom_point(aes(y=diff_Tree), color='dark green', alpha=0.6) + 
    geom_hline(yintercept=c(rmseTree, -1*rmseTree), color='dark green') +
    geom_point(aes(y=diff_GLM), color='dark blue', alpha=0.6) + 
    geom_hline(yintercept=c(rmseGLM, -1*rmseGLM), color='dark blue') +
    theme_bw() + coord_cartesian(ylim=c(-4,4)) + 
    labs(x='User', y='Difference from Model')
```

Here is a table of the root mean square errors (RMSE), a measure of the goodness-of-fit:
```{r}
modelSummary %>% kable
```


## Conclusions

