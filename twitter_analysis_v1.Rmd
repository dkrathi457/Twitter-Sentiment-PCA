---
title: "Twitter Analysis"
author: "David Rodriguez"
date: "February 24, 2016"
output: html_document
---

## Introduction

Twitter is a powerful two-edged tool that enables users to communicate with others and also empowers data scientists with large quantities of data they can use.

## Experiment Setup

Load up required packages:
```{r message=F, warning=F}
library(twitteR)
library(tm)
library(rjson)
library(wordcloud)
library(dplyr)
library(caret)
library(knitr)
library(RColorBrewer)
```

Read in my application credentials:
```{r eval=FALSE}
secrets <- fromJSON(file='twitter_secrets.json.nogit')

setup_twitter_oauth(secrets$api_key,
                    secrets$api_secret,
                    secrets$access_token,
                    secrets$access_token_secret)
```

Perform a twitter search and extract the information we want:
```{r eval=F}
searchstring <- 'python'
numtweets <- 1000
st <- searchTwitter(searchstring, n=numtweets, resultType = 'recent', lang = 'en')

st_text <- sapply(st, function(x) x$getText())
st_user <- sapply(st, function(x) x$getScreenName())
st_rts <- sapply(st, function(x) x$isRetweet)
st_lat <- sapply(st, function(x) as.numeric(x$latitude[1]))
st_lon <- sapply(st, function(x) as.numeric(x$longitude[1]))
st_time <- sapply(st, function(x) format(x$created, format='%F %T'))

statuses <- data.frame('text'=st_text, 'user'=st_user, 'RT'=st_rts,
                       'latitude'=st_lat, 'longitude'=st_lon,
                       'time'=st_time)
```

Remove retweets for clarity:
```{r eval=F}
statuses <-
    statuses %>%
    filter(!RT)
```

To save time, we'll load up saved tweets:
```{r loadtweets}
#saveRDS(statuses, file='tweet_data.Rda')
statuses <- readRDS(file='data/tweet_data.Rda')
```

Total number of tweets to process is `r nrow(statuses)`

## Text Analysis

Gather the tweets:
```{r gathercorpus}
textdata <- Corpus(VectorSource(statuses$text))

textdata <- 
    textdata %>%
    tm_map(removeWords, stopwords("english"), mc.cores=1) %>%
    tm_map(removePunctuation, mc.cores=1) %>%
    tm_map(content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
           mc.cores=1) %>%
    tm_map(content_transformer(tolower), mc.cores=1) %>%
    tm_map(removeNumbers, mc.cores=1) %>%
    tm_map(removeWords, c('trump','realdonaldtrump'), mc.cores=1) %>%
    tm_map(stripWhitespace, mc.cores=1)

#str_replace_all(tweets$text, "@\\w+", "") # removing twitter handles
```

```{r eval=F}
library("RWeka")
#NGramTokenizer(textdata, Weka_control(min = 2, max = 3))

#create fucntion to send to TermDocumentMatrix
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
options(mc.cores=1)
tdm <- TermDocumentMatrix(textdata, control = list(tokenize = BigramTokenizer))

inspect(tdm[1:5,1:3])
dtm <- DocumentTermMatrix(textdata, control = list(tokenize = BigramTokenizer))
inspect(head(dtm))

head(findFreqTerms(tdm), 10)
```

```{r eval=F}
library(syuzhet)

sentiments <- sapply(textdata, function(x) get_nrc_sentiment(as.character(x[[1]])))
dim(sentiments)

sentiments2 <- as.data.frame(aperm(sentiments)) # transpose and save as dataframe
#sentiments2['positivity'] <- sentiments2$positive - sentiments2$negative # not working
```


A quick wordcloud:
```{r}
pal2 <- brewer.pal(8,"Dark2")
wordcloud(textdata, max.words = 100, colors= pal2)
```

Further processing to get word counts:
```{r termmatrix, message=F, results='hide'}
dtm <- DocumentTermMatrix(textdata)
dtm <- inspect(dtm)

words <- data.frame(term = colnames(dtm))
words$count <- colSums(dtm)
```

Sort in descending order:
```{r}
words <-
    words %>%
    arrange(desc(count))
head(words)
```

Convert tweets to data frame:
```{r}
tweets <- as.data.frame(dtm)
ind <- data.frame('id'=seq.int(nrow(tweets)))
tweets <- cbind(ind, tweets)
rm(dtm, textdata) # clearing up memory
```

Select only the top 100 words (not the search term) to process:
```{r}
words_100 <- as.character(words[2:101,'term'])
```

Use only those variables to process the tweets:
```{r}
tweets <- tweets[,c('id',words_100)]
head(tweets[,1:10])
```

## PCA

Perform a principal component analysis on the tweet data set:
```{r runpca}
trans <- preProcess(tweets[,2:ncol(tweets)], method=c("pca"), thresh = 0.95)
pca <- predict(trans, tweets[,2:ncol(tweets)])
```

Examine reprojected data
```{r}
ggplot(pca, aes(x=PC1, y=PC2)) + 
    geom_point(color='dark blue') +
    theme_bw()
```

Look at loading factors:
```{r}
loadings <- trans$rotation 
load_sqr <- loadings^2

load_sqr <- data.frame(load_sqr)
temp <- data.frame('term'=rownames(load_sqr))
load_sqr <- cbind(temp, load_sqr)
load_sqr %>%
    select(term, PC1) %>%
    arrange(desc(PC1)) %>%
    head(10)

load_sqr %>%
    select(term, PC2) %>%
    arrange(desc(PC2)) %>%
    head(10)
```

Join the PCA information (first five components) to the original status array:
```{r}
statuses <- cbind(statuses, pca[,1:5])
```

I can now examine the tweets based on where they fall in principal component space.

I've created a function to sample tweets across the PC spectrum.
```{r}
set.seed(42)
tweet_check <- function(text, pc, numbreaks=5){
    cuts <- cut(pc, numbreaks)
    temp <- data.frame(text=text, pc=pc, pc_val=cuts)
    temp <- temp %>%
        group_by(pc_val) %>%
        summarise(text=sample(text,1))
    return(temp)
}
```

Here are the results for PC1:
```{r}
tweet_check(statuses$text, statuses$PC1, 10) %>% kable(format='html')
```


And here are the results for PC2:
```{r}
tweet_check(statuses$text, statuses$PC2, 10) %>% kable(format='html')
```


Saving the processed tweets to continue work elsewhere:
```{r}
saveRDS(statuses, file='data/proc_tweets.Rda')
```


## Data Prediction
