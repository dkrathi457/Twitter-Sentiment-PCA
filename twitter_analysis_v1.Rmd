---
title: "Twitter Analysis"
author: "David Rodriguez"
date: "February 24, 2016"
output: html_document
---

## Introduction

Twitter is a powerful two-edged tool that enables users to communicate with others and also empowers data scientists with large quantities of data they can use.

## Experiment Setup

Load up required packages:
```{r message=F, warning=F}
library(twitteR)
library(tm)
library(rjson)
library(wordcloud)
library(dplyr)
library(caret)
library(knitr)
library(RColorBrewer)
library(stringr)
library(syuzhet) # for sentiment analysis
```

Read in my application credentials:
```{r eval=FALSE}
secrets <- fromJSON(file='twitter_secrets.json.nogit')

setup_twitter_oauth(secrets$api_key,
                    secrets$api_secret,
                    secrets$access_token,
                    secrets$access_token_secret)
```

Perform a twitter search and extract the information we want:
```{r eval=F}
searchstring <- 'politics'
numtweets <- 10000
st <- searchTwitter(searchstring, n=numtweets, resultType = 'recent', lang = 'en')

statuses <- data.frame(text=sapply(st, function(x) x$getText()),
                       user=sapply(st, function(x) x$getScreenName()),
                       RT=sapply(st, function(x) x$isRetweet),
                       latitude=sapply(st, function(x) as.numeric(x$latitude[1])),
                       longitude=sapply(st, function(x) as.numeric(x$longitude[1])),
                       time=sapply(st, function(x) format(x$created, format='%F %T'))
                       )
```

Remove retweets for clarity:
```{r eval=F}
statuses <-
    statuses %>%
    filter(!RT)
```

Save tweets for future use:
```{r savetweets, eval=F}
today <- format(Sys.time(), '%Y-%m-%d')
savename <- paste0('data/tweets_',searchstring,'_',
                   nrow(statuses),'_',today,'.Rda')
saveRDS(statuses, file=savename)
```

```{r loadtweets, eval=T}
files <- list.files('data','tweets_')
files
selectedfile <- paste0('data/',files[1])
statuses <- readRDS(file=selectedfile)
searchstring <- 'politics'
```

Total number of tweets to process is `r nrow(statuses)`

## Text Analysis

Gather the tweets:
```{r gathercorpus}
textdata <- Corpus(VectorSource(statuses$text))

textdata <- 
    textdata %>%
    tm_map(removeWords, stopwords("english"), mc.cores=1) %>%
    tm_map(removePunctuation, mc.cores=1) %>%
    tm_map(content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
           mc.cores=1) %>%
    tm_map(content_transformer(tolower), mc.cores=1) %>%
    tm_map(content_transformer(function(x) str_replace_all(x, "@\\w+", "")), 
           mc.cores=1) %>% # remove twitter handles
    tm_map(removeNumbers, mc.cores=1) %>%
    tm_map(removeWords, c('trump','realdonaldtrump'), mc.cores=1) %>%
    tm_map(stripWhitespace, mc.cores=1)
```

```{r eval=F}
library("RWeka")
#NGramTokenizer(textdata, Weka_control(min = 2, max = 3))

#create fucntion to send to TermDocumentMatrix
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
options(mc.cores=1)
tdm <- TermDocumentMatrix(textdata, control = list(tokenize = BigramTokenizer))

inspect(tdm[1:5,1:3])
dtm <- DocumentTermMatrix(textdata, control = list(tokenize = BigramTokenizer))
inspect(head(dtm))

head(findFreqTerms(tdm), 10)
```

```{r sentiment, eval=T}
sentiments <- sapply(textdata, function(x) get_nrc_sentiment(as.character(x)))

sentiments <- as.data.frame(aperm(sentiments)) # transpose and save as dataframe
sentiments <- as.data.frame(lapply(sentiments, as.numeric)) # a bit more to organize
sentiments <-
    sentiments %>%
    mutate(positivity = positive - negative)
```

A quick wordcloud:
```{r}
pal2 <- brewer.pal(8,"Dark2")
wordcloud(textdata, max.words = 100, colors= pal2)
```

Further processing to get word counts:
```{r termmatrix, message=F, results='hide'}
dtm <- DocumentTermMatrix(textdata)
dtm <- inspect(dtm)

words <- data.frame(term = colnames(dtm))
words$count <- colSums(dtm)
```

Sort in descending order:
```{r}
words <-
    words %>%
    arrange(desc(count))
head(words)
```

Convert tweets to data frame:
```{r}
tweets <- as.data.frame(dtm)
ind <- data.frame('id'=seq.int(nrow(tweets)))
tweets <- cbind(ind, tweets)
rm(dtm, textdata) # clearing up memory
```

Select only the top 100 words (not the search term) to process:
```{r}
words_100 <- as.character(words[2:101,'term'])
```

Use only those variables to process the tweets:
```{r}
tweets <- tweets[,c('id',words_100)]
head(tweets[,1:10])
```

## PCA

Perform a principal component analysis on the tweet data set:
```{r runpca}
trans <- preProcess(tweets[,2:ncol(tweets)], method=c("pca"), thresh = 0.95)
pca <- predict(trans, tweets[,2:ncol(tweets)])
```

Join the PCA information (first five components) to the original status array.
Examine reprojected data
```{r}
statuses <- cbind(statuses, pca[,1:5], sentiments)
pal2 <- brewer.pal(10,"RdBu")
ggplot(statuses, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=positivity), size=4, alpha=0.7, pch=21, stroke=1.3) + 
    scale_fill_gradientn(colours = pal2, limits=c(-5,5)) + theme_bw()
```

Look at loading factors:
```{r}
loadings <- trans$rotation 
load_sqr <- loadings^2

load_sqr <- data.frame(load_sqr)
temp <- data.frame('term'=rownames(load_sqr))
load_sqr <- cbind(temp, load_sqr)
load_sqr %>%
    select(term, PC1) %>%
    arrange(desc(PC1)) %>%
    head(10)

load_sqr %>%
    select(term, PC2) %>%
    arrange(desc(PC2)) %>%
    head(10)
```


I've created a function to sample tweets across the PC spectrum.
```{r}
set.seed(42)
tweet_check <- function(text, pc, numbreaks=5){
    cuts <- cut(pc, numbreaks)
    temp <- data.frame(text=text, pc=pc, pc_val=cuts)
    temp <- temp %>%
        group_by(pc_val) %>%
        summarise(text=iconv(sample(text,1), to='UTF-8-MAC', sub='byte'))
    return(temp)
}
```

Here are the results for PC1:
```{r}
tweet_check(statuses$text, statuses$PC1, 10) %>% kable(format='html')
```

And here are the results for PC2:
```{r}
tweet_check(statuses$text, statuses$PC2, 10) %>% kable(format='html')
```

Finally, let's have a look at tweets grouped by positivity score:
```{r}
tweet_check(statuses$text, statuses$positivity, 10) %>% kable(format='html')
```


Saving the processed tweets to continue work elsewhere:
```{r}
today <- format(Sys.time(), '%Y-%m-%d')
savename <- paste0('data/proc_',searchstring,'_',
                   nrow(statuses),'_',today,'.Rda')
saveRDS(statuses, file=savename)
```


## Data Prediction
